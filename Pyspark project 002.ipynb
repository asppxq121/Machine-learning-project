{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This Notebook was create only for learning Apache Spark.\n",
    "In this notebook I modified code from reference to learn data science project in spark. \n",
    "Since we used different version of spark, I have tried my best to modify but the problem is .count function is cannot be used to result due to some problem that we cannot clarify.\n",
    "\n",
    "Ref:\n",
    "[1] Spark for Data Science:Bikramaditya Singhal, Srinivas Duvvuri\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf();\n",
    "sc = SparkContext(appName=\"put_it_all_AppName\", conf =conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .appName('strating spark')\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data = spark.read.csv(\"C:/Users/New/Downloads/Data science in Action/Data set for spark/Spark-for-Data-Science-master/B04897_Spark for Data Science_Chapter 10_Code_Spark2.0_V1//Oscars.txt\",\n",
    "                           sep=\"\\t\",\n",
    "                           header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------+--------------+-------------+-------------+--------------------+---------------+\n",
      "| _unit_id|        birthplace|date_of_birth|race_ethnicity|year_of_award|        award|               movie|         person|\n",
      "+---------+------------------+-------------+--------------+-------------+-------------+--------------------+---------------+\n",
      "|670454353| Chisinau, Moldova|  30-Sep-1895|         White|         1927|Best Director| Two Arabian Knights|Lewis Milestone|\n",
      "|670454354| Glasgow, Scotland|   2-Feb-1886|         White|         1930|Best Director|     The Divine Lady|    Frank Lloyd|\n",
      "|670454355| Chisinau, Moldova|  30-Sep-1895|         White|         1931|Best Director|All Quiet on the ...|Lewis Milestone|\n",
      "|670454356|       Chicago, Il|  23-Feb-1899|         White|         1932|Best Director|              Skippy|  Norman Taurog|\n",
      "|670454357|Salt Lake City, Ut|  23-Apr-1894|         White|         1933|Best Director|            Bad Girl|  Frank Borzage|\n",
      "+---------+------------------+-------------+--------------+-------------+-------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards = init_data.select(\"birthplace\", \"date_of_birth\",\"race_ethnicity\",\"year_of_award\",\"award\")\\\n",
    "                  .toDF(\"birthplace\",\"date_of_birth\",\"race\",\"award_year\",\"award\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------+-----+----------+-------------+\n",
      "|birthplace               |date_of_birth|race |award_year|award        |\n",
      "+-------------------------+-------------+-----+----------+-------------+\n",
      "|Chisinau, Moldova        |30-Sep-1895  |White|1927      |Best Director|\n",
      "|Glasgow, Scotland        |2-Feb-1886   |White|1930      |Best Director|\n",
      "|Chisinau, Moldova        |30-Sep-1895  |White|1931      |Best Director|\n",
      "|Chicago, Il              |23-Feb-1899  |White|1932      |Best Director|\n",
      "|Salt Lake City, Ut       |23-Apr-1894  |White|1933      |Best Director|\n",
      "|Glasgow, Scotland        |2-Feb-1886   |White|1934      |Best Director|\n",
      "|Bisacquino, Sicily, Italy|18-May-1897  |White|1935      |Best Director|\n",
      "|Cape Elizabeth, Me       |1-Feb-1894   |White|1936      |Best Director|\n",
      "|Bisacquino, Sicily, Italy|18-May-1897  |White|1937      |Best Director|\n",
      "|Los Angeles, Ca          |3-Oct-1898   |White|1938      |Best Director|\n",
      "+-------------------------+-------------+-----+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "awards.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register temporary view of this dataset\n",
    "awards.createOrReplaceTempView(\"award_SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|award                  |\n",
      "+-----------------------+\n",
      "|Best Supporting Actress|\n",
      "|Best Director          |\n",
      "|Best Actress           |\n",
      "|Best Actor             |\n",
      "|Best Supporting Actor  |\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore data\n",
    "awards.select(\"award\").distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|length(date_of_birth)|\n",
      "+---------------------+\n",
      "|                   15|\n",
      "|                    9|\n",
      "|                    4|\n",
      "|                    8|\n",
      "|                   10|\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check DOB quality\n",
    "# SQL Mode\n",
    "spark.sql(\"SELECT distinct(length(date_of_birth)) FROM award_SQL \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|date_of_birth|\n",
      "+-------------+\n",
      "|         1972|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the value with unexpected length 4. Note that length varies based on month name\n",
    "spark.sql(\"SELECT date_of_birth FROM award_SQL WHERE length(date_of_birth) = 4\").show()\n",
    "\n",
    "# This is an invalid date. We can either drop this record or give some meaningful value like 01-01-1972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.fncleanBirthplace(s)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fncleanDate(s):\n",
    "    cleanedDate = s\n",
    "    dateArray = s.split(\"-\")\n",
    "    try:\n",
    "        yr = int(dateArray[2])\n",
    "        if (yr < 100):\n",
    "            yr = yr + 1900\n",
    "            cleanedDate = \"{0}-{1}-{2}\".format(int(dateArray[0]), dateArray[1], yr)\n",
    "    except: None\n",
    "    return cleanedDate\n",
    "\n",
    "\n",
    "def fncleanBirthplace(s):\n",
    "    strArray = s.split(\",\")\n",
    "    if (s == \"New York City\"):\n",
    "        s += \" USA\"  # //Append USA\n",
    "        cleanedBirthplace = s\n",
    "    #//Append country if last element length is 2\n",
    "    elif (len(strArray[len(strArray)-1])-1) == 2:\n",
    "        s += \" USA\"\n",
    "        cleanedBirthplace = \"\".join(s)\n",
    "    else: cleanedBirthplace = s\n",
    "    return cleanedBirthplace\n",
    "\n",
    "#regist UDF\n",
    "spark.udf.register(\"cleanDateUDF\",fncleanDate)\n",
    "spark.udf.register(\"cleanBirthplaceUDF\",fncleanBirthplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+----+-----+-------------+\n",
      "|        dob|          birthplace| country| age| race|        award|\n",
      "+-----------+--------------------+--------+----+-----+-------------+\n",
      "|30-Sep-1895|   Chisinau, Moldova| Moldova|32.0|White|Best Director|\n",
      "| 2-Feb-1886|   Glasgow, Scotland|Scotland|44.0|White|Best Director|\n",
      "|30-Sep-1895|   Chisinau, Moldova| Moldova|36.0|White|Best Director|\n",
      "|23-Feb-1899|     Chicago, Il USA|     USA|33.0|White|Best Director|\n",
      "|23-Apr-1894|Salt Lake City, U...|     USA|39.0|White|Best Director|\n",
      "+-----------+--------------------+--------+----+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring_index\n",
    "\n",
    "cleaned_df = spark.sql (\"\"\" \n",
    "            SELECT  cleanDateUDF(date_of_birth) AS dob,\n",
    "                    cleanBirthplaceUDF(birthplace) AS birthplace,\n",
    "                    substring_index(cleanBirthplaceUDF(birthplace),' ',-1) AS country,\n",
    "                    (award_year - substring_index(cleanDateUDF(date_of_birth),'-',-1)) AS age, \n",
    "                    race, \n",
    "                    award\n",
    "            \n",
    "            FROM award_SQL\n",
    "            \"\"\")\n",
    "\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT cleanDateUDF (date_of_birth) AS dob FROM award_SQL\").show(5)\n",
    "# spark.sql(\"SELECT cleanBirthplaceUDF(birthplace) AS birthplace FROM award_SQL\").show(5, truncate = False)\n",
    "# spark.sql(\"SELECT (award_year - substring_index(cleanDateUDF(date_of_birth),'-',-1)) AS age FROM award_SQL\").show(5)\n",
    "# spark.sql(\"SELECT substring_index(cleanBirthplaceUDF(birthplace),' ',-1) AS country FROM award_SQL\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-----+\n",
      "|award                  |country  |count|\n",
      "+-----------------------+---------+-----+\n",
      "|Best Actress           |Africa   |1    |\n",
      "|Best Actor             |Australia|1    |\n",
      "|Best Actress           |Australia|1    |\n",
      "|Best Supporting Actor  |Australia|1    |\n",
      "|Best Supporting Actress|Australia|1    |\n",
      "+-----------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.na.drop\n",
    "\n",
    "cleaned_df.groupBy(\"award\",\"country\")\\\n",
    "          .count()\\\n",
    "          .sort(\"country\",\"award\",\"count\")\\\n",
    "          .show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-register\n",
    "cleaned_df.createOrReplaceTempView(\"award_SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+\n",
      "|country_count|race_count|award_count|\n",
      "+-------------+----------+-----------+\n",
      "|           35|         6|          5|\n",
      "+-------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(distinct country) country_count, count(distinct race) race_count, count(distinct award) award_count from award_SQL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = cleaned_df.select(\"country\")\\\n",
    "                    .groupBy(\"country\")\\\n",
    "                    .count()\\\n",
    "                    .withColumnRenamed(\"count\", \"freq\")\n",
    "\n",
    "df_main = df_main.orderBy(\"count\", ascending = False)\\\n",
    "                 .filter(df_main.freq > 6)\n",
    "\n",
    "# #  based on book \n",
    "# top_countries = spark.sql(\"\"\"\n",
    "#                              SELECT country, count(*) freq \n",
    "#                              FROM award_SQL \n",
    "#                              GROUP BY country \n",
    "#                              ORDER BY freq DESC \n",
    "#                              LIMIT 6\n",
    "#                          \"\"\")\n",
    "# top_countries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(country='USA'), Row(country='England'), Row(country='France'), Row(country='Canada'), Row(country='Italy'), Row(country='Austria')]\n",
      "\n",
      "[['USA']\n",
      " ['England']\n",
      " ['France']\n",
      " ['Canada']\n",
      " ['Italy']\n",
      " ['Austria']]\n",
      "\n",
      "[['USA'], ['England'], ['France'], ['Canada'], ['Italy'], ['Austria']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "RDD_list = df_main.select(df_main.columns[:1]).collect() # RDD List is notreal  Python List\n",
    "np_list = np.array(RDD_list) \n",
    "py_list = np_list.tolist()\n",
    "\n",
    "print(RDD_list)\n",
    "print(\"\")\n",
    "print(np_list)\n",
    "print(\"\")\n",
    "print(py_list)\n",
    "# npp = np.array().ะนสรหะ\n",
    "# npp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USA', 'England', 'France', 'Canada', 'Italy', 'Austria']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_countries = [x for row in py_list for x in row]\n",
    "# for each row in py_list \n",
    "#     for x in each row\n",
    "# return to x\n",
    "\n",
    "top_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+-------+----+-----+-------------+\n",
      "|dob        |birthplace               |country|age |race |award        |\n",
      "+-----------+-------------------------+-------+----+-----+-------------+\n",
      "|30-Sep-1895|Chisinau, Moldova        |Others |32.0|White|Best Director|\n",
      "|2-Feb-1886 |Glasgow, Scotland        |Others |44.0|White|Best Director|\n",
      "|30-Sep-1895|Chisinau, Moldova        |Others |36.0|White|Best Director|\n",
      "|23-Feb-1899|Chicago, Il USA          |USA    |33.0|White|Best Director|\n",
      "|23-Apr-1894|Salt Lake City, Ut USA   |USA    |39.0|White|Best Director|\n",
      "|2-Feb-1886 |Glasgow, Scotland        |Others |48.0|White|Best Director|\n",
      "|18-May-1897|Bisacquino, Sicily, Italy|Italy  |38.0|White|Best Director|\n",
      "|1-Feb-1894 |Cape Elizabeth, Me USA   |USA    |42.0|White|Best Director|\n",
      "|18-May-1897|Bisacquino, Sicily, Italy|Italy  |40.0|White|Best Director|\n",
      "|3-Oct-1898 |Los Angeles, Ca USA      |USA    |40.0|White|Best Director|\n",
      "+-----------+-------------------------+-------+----+-----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setCountry = udf(lambda s: s if s in top_countries else \"Others\", StringType())\n",
    "cleaned_df = cleaned_df.withColumn(\"country\", setCountry(cleaned_df.country))\n",
    "cleaned_df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, Bucketizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Pipeline\n",
    "''' [raceIdxer] => \n",
    "        [awardIdxer] => \n",
    "            [countryIdxer] => \n",
    "                [bucketizer] => \n",
    "                    [TrainTest_split] => \n",
    "                        [VectorAssembler]'''\n",
    "\n",
    "# calculate the frequency of String in column the rank them; 0 is the most frequent words\n",
    "raceIdxer = StringIndexer(inputCol= \"race\", outputCol=\"raceIdx\")\n",
    "awardIdxer = StringIndexer(inputCol = \"award\", outputCol=\"awardIdx\") \n",
    "countryIdxer = StringIndexer(inputCol = \"country\", outputCol = \"countryIdx\")\n",
    "\n",
    "splits = [-float(\"inf\"), 35.0, 45.0, 55.0, float(\"inf\")] # Range finder, create interval (bucket) to group the data into 4 group\n",
    "'''               |___1___||__2__||_3_||__4___| ''' # Visualize bucket\n",
    "bucketizer = Bucketizer(splits = splits, inputCol = \"age\", outputCol = \"age_buckets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data preparation pipeline\n",
    "dp_pipeline = Pipeline(stages = [raceIdxer, awardIdxer, countryIdxer, bucketizer])\n",
    "cleaned_df = dp_pipeline.fit(cleaned_df)\\\n",
    "                        .transform(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.createOrReplaceTempView(\"cleaned_df_SQL\") # turn SQL mode ON\n",
    "TrainTest_split = spark.sql(\"SELECT * FROM cleaned_df_SQL\").randomSplit([0.7, 0.3])# trainData, testData = cleaned_df1.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataFrame[dob: string, birthplace: string, country: string, age: double, race: string, award: string, raceIdx: double, awardIdx: double, countryIdx: double, age_buckets: double], DataFrame[dob: string, birthplace: string, country: string, age: double, race: string, award: string, raceIdx: double, awardIdx: double, countryIdx: double, age_buckets: double]]\n",
      "+-------+----------+-----------------+-------+-----------------+-----+-----------------------+-------------------+------------------+------------------+------------------+\n",
      "|summary|dob       |birthplace       |country|age              |race |award                  |raceIdx            |awardIdx          |countryIdx        |age_buckets       |\n",
      "+-------+----------+-----------------+-------+-----------------+-----+-----------------------+-------------------+------------------+------------------+------------------+\n",
      "|count  |309       |309              |309    |309              |309  |309                    |309                |309               |309               |309               |\n",
      "|mean   |null      |null             |null   |43.68608414239482|null |null                   |0.11326860841423948|1.8996763754045307|0.7961165048543689|1.3754045307443366|\n",
      "|stddev |null      |null             |null   |12.33074372392456|null |null                   |0.47343590298861116|1.4140872505153055|1.3437630576904982|1.0575226417300074|\n",
      "|min    |1-Apr-1885|Arlington, Va USA|Austria|11.0             |Asian|Best Actor             |0.0                |0.0               |0.0               |0.0               |\n",
      "|max    |9-Sep-1923|Yonkers, Ny USA  |USA    |83.0             |White|Best Supporting Actress|4.0                |4.0               |6.0               |3.0               |\n",
      "+-------+----------+-----------------+-------+-----------------+-----+-----------------------+-------------------+------------------+------------------+------------------+\n",
      "\n",
      "+-------+----------+------------------------------+-------+------------------+-----+-----------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|dob       |birthplace                    |country|age               |race |award                  |raceIdx           |awardIdx          |countryIdx        |age_buckets      |\n",
      "+-------+----------+------------------------------+-------+------------------+-----+-----------------------+------------------+------------------+------------------+-----------------+\n",
      "|count  |132       |132                           |132    |131               |132  |132                    |132               |132               |132               |131              |\n",
      "|mean   |1972.0    |null                          |null   |44.25954198473283 |null |null                   |0.1590909090909091|1.9924242424242424|0.5303030303030303|1.366412213740458|\n",
      "|stddev |NaN       |null                          |null   |12.888031291001706|null |null                   |0.697099174327457 |1.4222668290965226|1.0443551934409496|0.986104276513072|\n",
      "|min    |1-Aug-1965|Baldwin, Ny USA               |Austria|21.0              |Asian|Best Actor             |0.0               |0.0               |0.0               |0.0              |\n",
      "|max    |9-Jun-1981|York, North Yorkshire, England|USA    |82.0              |White|Best Supporting Actress|5.0               |4.0               |6.0               |3.0              |\n",
      "+-------+----------+------------------------------+-------+------------------+-----+-----------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(TrainTest_split)  # split_data consist of 2 DF. We will assign them as trainData, testData later. \n",
    "trainData = TrainTest_split[0]\n",
    "testData = TrainTest_split[1]\n",
    "trainData.describe().show(truncate = False)\n",
    "testData.describe().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  raceIdx, age_buckets,countryIdx features via VectorAssembler\n",
    "assembler = VectorAssembler(inputCols = [\"raceIdx\", \"age_buckets\",\"countryIdx\"], outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = assembler.transform(trainData)\n",
    "testData = assembler.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+\n",
      "|       dob|          birthplace|country| age| race|        award|raceIdx|awardIdx|countryIdx|age_buckets|     features|\n",
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+\n",
      "|1-Apr-1885| Kansas City, Mo USA|    USA|47.0|White|   Best Actor|    0.0|     2.0|       0.0|        2.0|[0.0,2.0,0.0]|\n",
      "|1-Dec-1935|    Brooklyn, Ny USA|    USA|43.0|White|Best Director|    0.0|     1.0|       0.0|        1.0|[0.0,1.0,0.0]|\n",
      "|1-Feb-1894|Cape Elizabeth, M...|    USA|42.0|White|Best Director|    0.0|     1.0|       0.0|        1.0|[0.0,1.0,0.0]|\n",
      "|1-Feb-1894|Cape Elizabeth, M...|    USA|47.0|White|Best Director|    0.0|     1.0|       0.0|        2.0|[0.0,2.0,0.0]|\n",
      "|1-Feb-1894|Cape Elizabeth, M...|    USA|48.0|White|Best Director|    0.0|     1.0|       0.0|        2.0|[0.0,2.0,0.0]|\n",
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+\n",
      "|       dob|          birthplace|country| age| race|        award|raceIdx|awardIdx|countryIdx|age_buckets|     features|\n",
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+\n",
      "|1-Aug-1965|Reading, Berkshir...|England|35.0|White|Best Director|    0.0|     1.0|       2.0|        1.0|[0.0,1.0,2.0]|\n",
      "|1-Feb-1901|       Cadiz, Oh USA|    USA|34.0|White|   Best Actor|    0.0|     2.0|       0.0|        0.0|    (3,[],[])|\n",
      "|1-Jul-1899|Victoria Hotel, S...|England|35.0|White|   Best Actor|    0.0|     2.0|       2.0|        1.0|[0.0,1.0,2.0]|\n",
      "|1-Jul-1916|        Tokyo, Japan| Others|31.0|White| Best Actress|    0.0|     0.0|       1.0|        0.0|[0.0,0.0,1.0]|\n",
      "|1-Jul-1934|   Lafayette, In USA|    USA|52.0|White|Best Director|    0.0|     1.0|       0.0|        2.0|[0.0,2.0,0.0]|\n",
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtreeModel = DecisionTreeClassifier(labelCol = \"awardIdx\", featuresCol = \"features\").fit(trainData)\n",
    "dtree_predictions = dtreeModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "|       dob|          birthplace|country| age| race|        award|raceIdx|awardIdx|countryIdx|age_buckets|     features|       rawPrediction|         probability|prediction|\n",
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "|1-Aug-1965|Reading, Berkshir...|England|35.0|White|Best Director|    0.0|     1.0|       2.0|        1.0|[0.0,1.0,2.0]|[7.0,3.0,2.0,3.0,...|[0.46666666666666...|       0.0|\n",
      "|1-Feb-1901|       Cadiz, Oh USA|    USA|34.0|White|   Best Actor|    0.0|     2.0|       0.0|        0.0|    (3,[],[])|[39.0,0.0,4.0,20....|[0.56521739130434...|       0.0|\n",
      "|1-Jul-1899|Victoria Hotel, S...|England|35.0|White|   Best Actor|    0.0|     2.0|       2.0|        1.0|[0.0,1.0,2.0]|[7.0,3.0,2.0,3.0,...|[0.46666666666666...|       0.0|\n",
      "|1-Jul-1916|        Tokyo, Japan| Others|31.0|White| Best Actress|    0.0|     0.0|       1.0|        0.0|[0.0,0.0,1.0]|[39.0,0.0,4.0,20....|[0.56521739130434...|       0.0|\n",
      "|1-Jul-1934|   Lafayette, In USA|    USA|52.0|White|Best Director|    0.0|     1.0|       0.0|        2.0|[0.0,2.0,0.0]|[9.0,25.0,27.0,17...|[0.07964601769911...|       4.0|\n",
      "+----------+--------------------+-------+----+-----+-------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+----+-----+--------------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "|       dob|          birthplace|country| age| race|               award|raceIdx|awardIdx|countryIdx|age_buckets|     features|       rawPrediction|         probability|prediction|\n",
      "+----------+--------------------+-------+----+-----+--------------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "|1-Aug-1965|Reading, Berkshir...|England|35.0|White|       Best Director|    0.0|     1.0|       2.0|        1.0|[0.0,1.0,2.0]|[7.0,3.0,2.0,3.0,...|[0.46666666666666...|       0.0|\n",
      "|1-Feb-1901|       Cadiz, Oh USA|    USA|34.0|White|          Best Actor|    0.0|     2.0|       0.0|        0.0|    (3,[],[])|[39.0,0.0,4.0,20....|[0.56521739130434...|       0.0|\n",
      "|1-Jul-1899|Victoria Hotel, S...|England|35.0|White|          Best Actor|    0.0|     2.0|       2.0|        1.0|[0.0,1.0,2.0]|[7.0,3.0,2.0,3.0,...|[0.46666666666666...|       0.0|\n",
      "|1-Jul-1934|   Lafayette, In USA|    USA|52.0|White|       Best Director|    0.0|     1.0|       0.0|        2.0|[0.0,2.0,0.0]|[9.0,25.0,27.0,17...|[0.07964601769911...|       4.0|\n",
      "|1-Jun-1937|     Memphis, Tn USA|    USA|68.0|Black|Best Supporting A...|    1.0|     4.0|       0.0|        3.0|[1.0,3.0,0.0]|[0.0,0.0,2.0,1.0,...|[0.0,0.0,0.666666...|       2.0|\n",
      "+----------+--------------------+-------+----+-----+--------------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_DT_full = dtree_predictions.filter(dtree_predictions[\"awardIdx\"] != dtree_predictions[\"prediction\"])\n",
    "result_DT_full.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|               award|awardIdx|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|       Best Director|     1.0|       0.0|\n",
      "|          Best Actor|     2.0|       0.0|\n",
      "|          Best Actor|     2.0|       0.0|\n",
      "|       Best Director|     1.0|       4.0|\n",
      "|Best Supporting A...|     4.0|       2.0|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_DT = dtree_predictions.select(\"award\",\"awardIdx\",\"prediction\")\\\n",
    "                             .filter(dtree_predictions.awardIdx != dtree_predictions.prediction)\n",
    "\n",
    "result_DT.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel = RandomForestClassifier(labelCol = \"awardIdx\", featuresCol = \"features\", numTrees =6).fit(trainData)\n",
    "RF_predictions = RFmodel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+----+-----+--------------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "|       dob|          birthplace|country| age| race|               award|raceIdx|awardIdx|countryIdx|age_buckets|     features|       rawPrediction|         probability|prediction|\n",
      "+----------+--------------------+-------+----+-----+--------------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "|1-Aug-1965|Reading, Berkshir...|England|35.0|White|       Best Director|    0.0|     1.0|       2.0|        1.0|[0.0,1.0,2.0]|[2.10069603272928...|[0.35011600545488...|       0.0|\n",
      "|1-Feb-1901|       Cadiz, Oh USA|    USA|34.0|White|          Best Actor|    0.0|     2.0|       0.0|        0.0|    (3,[],[])|[3.42929945768377...|[0.57154990961396...|       0.0|\n",
      "|1-Jul-1899|Victoria Hotel, S...|England|35.0|White|          Best Actor|    0.0|     2.0|       2.0|        1.0|[0.0,1.0,2.0]|[2.10069603272928...|[0.35011600545488...|       0.0|\n",
      "|1-Jul-1934|   Lafayette, In USA|    USA|52.0|White|       Best Director|    0.0|     1.0|       0.0|        2.0|[0.0,2.0,0.0]|[0.45018240785981...|[0.07503040130996...|       4.0|\n",
      "|1-Jun-1937|     Memphis, Tn USA|    USA|68.0|Black|Best Supporting A...|    1.0|     4.0|       0.0|        3.0|[1.0,3.0,0.0]|[0.22959531140590...|[0.03826588523431...|       2.0|\n",
      "+----------+--------------------+-------+----+-----+--------------------+-------+--------+----------+-----------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF_predictions.filter(RF_predictions[\"awardIdx\"] != RF_predictions[\"prediction\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|               award|awardIdx|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|       Best Director|     1.0|       0.0|\n",
      "|          Best Actor|     2.0|       0.0|\n",
      "|          Best Actor|     2.0|       0.0|\n",
      "|       Best Director|     1.0|       4.0|\n",
      "|Best Supporting A...|     4.0|       2.0|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF_predictions.select(\"award\",\"awardIdx\",\"prediction\")\\\n",
    "              .filter(RF_predictions[\"awardIdx\"] != RF_predictions[\"prediction\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(labelCol = \"awardIdx\", featuresCol=\"features\", maxIter = 30, tol = (1E-6), fitIntercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrModel = OneVsRest(classifier = classifier, labelCol = \"awardIdx\", featuresCol = \"features\").fit(trainData)\n",
    "OVR_predictions = ovrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2047.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 172.0 failed 1 times, most recent failure: Lost task 0.0 in stage 172.0 (TID 1352, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<raceIdx:double,age_buckets:double,countryIdx:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\r\n\t... 18 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<raceIdx:double,age_buckets:double,countryIdx:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\r\n\t... 18 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-d2fc4b352433>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mOVR_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;34m\"\"\"????????????????????????????????????????????????\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    531\u001b[0m         \"\"\"\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2047.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 172.0 failed 1 times, most recent failure: Lost task 0.0 in stage 172.0 (TID 1352, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<raceIdx:double,age_buckets:double,countryIdx:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\r\n\t... 18 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<raceIdx:double,age_buckets:double,countryIdx:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\r\n\t... 18 more\r\n"
     ]
    }
   ],
   "source": [
    "OVR_predictions.show()\n",
    "\"\"\"????????????????????????????????????????????????\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"awardIdx\") #Default metric is F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WeightedPrecision\n",
    "wp_eval = MulticlassClassificationEvaluator(labelCol=\"awardIdx\", metricName=\"weightedPrecision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "acc_eval = MulticlassClassificationEvaluator(labelCol=\"awardIdx\", metricName=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2092.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 170.0 failed 1 times, most recent failure: Lost task 0.0 in stage 170.0 (TID 1351, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<raceIdx:double,age_buckets:double,countryIdx:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:369)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1214)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1214)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1213)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:42)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:42)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure$lzycompute(MulticlassMetrics.scala:215)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:215)\r\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:84)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<raceIdx:double,age_buckets:double,countryIdx:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-c8b30f79ef59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compute measures for all models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf1_eval_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mf1_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdtree_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRF_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOVR_predictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mwp_eval_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mwp_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdtree_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRF_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOVR_predictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwr_eval_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mwr_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdtree_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRF_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOVR_predictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-c8b30f79ef59>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compute measures for all models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf1_eval_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mf1_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdtree_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRF_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOVR_predictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mwp_eval_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mwp_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdtree_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRF_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOVR_predictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwr_eval_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mwr_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdtree_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRF_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOVR_predictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \"\"\"\n\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2092.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 170.0 failed 1 times, most recent failure: Lost task 0.0 in stage 170.0 (TID 1351, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<raceIdx:double,age_buckets:double,countryIdx:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:370)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:369)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1214)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1214)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1213)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:42)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:42)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure$lzycompute(MulticlassMetrics.scala:215)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:215)\r\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:84)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<raceIdx:double,age_buckets:double,countryIdx:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "# Compute measures for all models\n",
    "f1_eval_list = [ f1_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]\n",
    "wp_eval_list = [ wp_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]\n",
    "wr_eval_list = [ wr_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''test: cleanDateUDF(date_of_birth) AS dob'''\n",
    "# s = \"30-Sep-95\"\n",
    "# dateArray = s.split(\"-\")\n",
    "# print(dateArray)\n",
    "# print(dateArray[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yr = int(dateArray[2])\n",
    "# if (yr < 100):\n",
    "#     yr = yr + 1900 # make it 4 digit\n",
    "#     cleanedDate = \"{0}-{1}-{2}\".format(int(dateArray[0]), dateArray[1], yr)\n",
    "#     print(cleanedDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"cleanBirthplaceUDF(birthplace)\"\"\"\n",
    "# # s = \"Chisinau, Moldova\"\n",
    "# s = \"Los Angeles, Ca\"\n",
    "# cleanedBirthplace = \"cleanedBirthplace\"\n",
    "# strArray = s.split(\",\")\n",
    "# strArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (s == \"New York City\"):\n",
    "#     s += \" USA\"  # //Append USA\n",
    "#     cleanedBirthplace = s\n",
    "#     print(cleanedBirthplace)\n",
    "# #//Append country if last element length is 2\n",
    "# elif (len(strArray[len(strArray)-1])-1) == 2:\n",
    "#     s += \" USA\"\n",
    "#     cleanedBirthplace = \"\".join(s)\n",
    "#     print(cleanedBirthplace)\n",
    "# else:\n",
    "#     cleanedBirthplace = s\n",
    "#     print(cleanedBirthplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(s)\n",
    "# print(len(s))\n",
    "# print(s[len(s)-1])\n",
    "# print(len(s[len(s)-1]))\n",
    "# print(\"\")\n",
    "\n",
    "# print(strArray)\n",
    "# print(len(strArray))\n",
    "# print(len(strArray[len(strArray)-1]))\n",
    "# print(len(strArray[len(strArray)-1])-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
